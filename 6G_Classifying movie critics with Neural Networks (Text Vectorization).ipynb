{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "celltoolbar": "Slideshow"
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Classifying movie critics with Neural Networks (Text Vectorization)",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nimport matplotlib.pyplot as plt\n\nimport os\nimport re\nimport string",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "* We will use a dataset containing the text of 50,000 reviews from the [Internet Movie Database (IMDb)](https://www.imdb.com/).\n\n* These are split into 25,000 reviews for training and 25,000 reviews for testing.\n\n* Our purpose is to build a model that will read a review and be able to decide whether it is positive or negative.\n\n* For more information see Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics. Available at <https://aclanthology.org/P11-1015>.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "* We will use a dataset containing the text of 50,000 reviews from the [Internet Movie Database (IMDb)](https://www.imdb.com/).\n\n* These are split into 25,000 reviews for training and 25,000 reviews for testing.\n\n* Our purpose is to build a model that will read a review and be able to decide whether it is positive or negative.\n\n* For more information see Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics. Available at <https://aclanthology.org/P11-1015>.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "dataset_dir = 'aclImdb'",
      "metadata": {},
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "* The data is contained in two subdirectories, `train` and `test`.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "os.listdir(dataset_dir)",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['test', 'README', 'train']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "* To see what is in the `train` directory.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "train_dir = os.path.join(dataset_dir, 'train')\nos.listdir(train_dir)",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['neg', 'pos']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": "* The `pos` directory contains positive reviews, each in a separate file.\n\n* Likewise, the `neg` directory contains negative reviews, each in a separate file.\n\n* To see a review.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\nwith open(sample_file) as f:\n    print(f.read())",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": "* To get the data into TensorFlow we will use the `text_dataset_from_directory()` function.\n\n* This assumes that our data is placed in directories, one for each distinct class, ie:\n\n ```\n main_directory/\n ...class_a/\n ......a_text_1.txt\n ......a_text_2.txt\n ...class_b/\n ......b_text_1.txt\n ......b_text_2.txt\n ```\n\n* This is exactly what we already have.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "* We will take the training subset.\n\n* We will keep 20% for validation (beyond audit data).",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "batch_size = 32\nseed = 42\n\nraw_train_ds = tf.keras.utils.text_dataset_from_directory(\n    'aclImdb/train', \n    batch_size=batch_size, \n    validation_split=0.2, \n    subset='training', \n    seed=seed)",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-03-18 15:07:19.688419: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": "* Let's look at some reviews and the class they belong to.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "for text_batch, label_batch in raw_train_ds.take(1):\n    for i in range(0, 2+1):\n        print(\"Review\", text_batch.numpy()[i])\n        print(\"Label\", label_batch.numpy()[i])",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
            "Label 0\n",
            "Review b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
            "Label 0\n",
            "Review b'Great documentary about the lives of NY firefighters during the worst terrorist attack of all time.. That reason alone is why this should be a must see collectors item.. What shocked me was not only the attacks, but the\"High Fat Diet\" and physical appearance of some of these firefighters. I think a lot of Doctors would agree with me that,in the physical shape they were in, some of these firefighters would NOT of made it to the 79th floor carrying over 60 lbs of gear. Having said that i now have a greater respect for firefighters and i realize becoming a firefighter is a life altering job. The French have a history of making great documentary\\'s and that is what this is, a Great Documentary.....'\n",
            "Label 1\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": "* To see what the two classes `0` and `1` correspond to we can use the `class_names` property.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\nprint(\"Label 1 corresponds to\", raw_train_ds.class_names[1])",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label 0 corresponds to neg\n",
            "Label 1 corresponds to pos\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": "* Having taken the training subset, we will now take the validation subset.\n\n* Note that we use the same seed to ensure that there will be no overlap between the training and validation data.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n    'aclImdb/train', \n    batch_size=batch_size, \n    validation_split=0.2, \n    subset='validation', \n    seed=seed)",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": "* Finally, we also get the control data.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n    'aclImdb/test', \n    batch_size=batch_size)",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": "* As we have seen before, reviews contain in addition to text the line break in HTML (`<br />`).\n\n* We will remove these.\n\n* We will also lowercase all characters and remove punctuation.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "def custom_standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n    return tf.strings.regex_replace(stripped_html,\n                                    '[%s]' % re.escape(string.punctuation),\n                                    '')",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": "* Next we will make a `TextVectorization` layer.\n\n* This layer will do the following preprocessing:\n\n * Will call `custom_standardization()`.\n\n * It will split each string corresponding to a review into individual tokens, using whitespace characters as separators.\n\n * It will assign each of the most frequently occurring word units an integer, creating a dictionary of size 10,000.\n\n * It will ensure that each resulting array of integers (representing each review) will have the same length.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "max_features = 10000\nsequence_length = 250\n\nvectorize_layer = layers.TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=max_features,\n    output_mode='int',\n    output_sequence_length=sequence_length)",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": "* We call the `adapt()` method of `vectorize_layer` on the training set to construct the mapping (vocabulary) between units and integers.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "# Make a text-only dataset (without labels), then call adapt\ntrain_text = raw_train_ds.map(lambda x, y: x)\nvectorize_layer.adapt(train_text)",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": "* To better see what `vectorize_layer` does, we'll write a helper function so we can call it on our data.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "def vectorize_text(text, label):\n    # text is a tensor with shape (), we need to make it with shape (1,)\n    text = tf.expand_dims(text, -1)\n    return vectorize_layer(text), label",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": "# retrieve a batch (of 32 reviews and labels) from the dataset\ntext_batch, label_batch = next(iter(raw_train_ds))\nfirst_review, first_label = text_batch[0], label_batch[0]\nprint(\"Review\", first_review)\nprint(\"Label\", raw_train_ds.class_names[first_label])\nprint(\"Vectorized review\", vectorize_text(first_review, first_label))",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review tf.Tensor(b'Great movie - especially the music - Etta James - \"At Last\". This speaks volumes when you have finally found that special someone.', shape=(), dtype=string)\n",
            "Label neg\n",
            "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
            "array([[  86,   17,  260,    2,  222,    1,  571,   31,  229,   11, 2418,\n",
            "           1,   51,   22,   25,  404,  251,   12,  306,  282,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": "* As we see, each word unit is represented by an integer.\n\n* The number 0 is used for padding, so that each review has the same length.\n\n* The number 1 is used for unknown words, i.e. words that are outside the 10,000 most frequent, which our dictionary holds.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "print(\"0 ---> \", vectorize_layer.get_vocabulary()[0])\nprint(\"1 ---> \",vectorize_layer.get_vocabulary()[1])\nprint(\"1287 ---> \", vectorize_layer.get_vocabulary()[1287])\nprint(\"9999 ---> \",vectorize_layer.get_vocabulary()[9999])\nprint(\"Vocabulary size: \", vectorize_layer.vocabulary_size())",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 --->  \n",
            "1 --->  [UNK]\n",
            "1287 --->  silent\n",
            "9999 --->  rushes\n",
            "Vocabulary size:  10000\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": "* To improve speed, we will use `cache()` and `prefetch()` methods.\n\n* With the `cache()` method, data the first time it is read can be cached in memory.\n\n* With the `prefetch()` method, data is fed to the neural network as the network is already processing the previous data, so no time is wasted feeding (since it can be done in parallel with processing).",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "AUTOTUNE = tf.data.AUTOTUNE\n\nraw_train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nraw_val_ds = raw_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\nraw_test_ds = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE)",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": "* We proceed to build our model.\n\n* The first layer we put in our model is `vectorize_layer`.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "model = tf.keras.Sequential()\n\nmodel.add(vectorize_layer)",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "source": "* Next we will add an *embedding layer*.\n\n* This layer converts the integer representing each word unit into a 16-dimensional *vector* (our choice).\n\n* Therefore we go from a \"word-number\" representation to a \"word-vector\" representation.\n\n* This vector representation, the integration, will somehow express the meaning of each verbal unit.\n\n* How is the vector representation of each word derived? The network will find out!\n\n* The embedding layer input is a sequence of integers, length 250.\n\n* The output of the layer will now be an array of dimensions $250 \\times 16$.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "embedding_dim = 16\n\nmodel.add(layers.Embedding(max_features, embedding_dim))",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": "* A dropout layer follows.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "model.add(layers.Dropout(0.2))",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "source": "* Each review is represented by a $250 \\times 16$ dimensional matrix.\n\n* From it we will produce a vector with 16 dimensions.\n\n* We will do this with a `GlobalAveragePooling1D` layer.\n\n* From the 250 vectors of 16 dimensions we will take their average.\n\n* Intuitively, this will be the vector representation of the \"average\" meaning of the word units of each review.\n\n* Even more intuitively, this will correspond to the meaning (in an ideal word) that sums up the entire critique.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "model.add(layers.GlobalAveragePooling1D())\nmodel.add(layers.Dropout(0.2))",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": "* Finally, we will add a densely connected neuron to the last layer to do the classification.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "model.add(layers.Dense(1))",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": "* Let's see briefly what we have:",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "model.summary()",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, 250)              0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 250, 16)           160000    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 250, 16)           0         \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 16)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160,017\n",
            "Trainable params: 160,017\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "source": "* As usual, we define optimizer, loss, and metric.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "source": "* We proceed to train for ten seasons.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "epochs = 10\nhistory = model.fit(\n    raw_train_ds,\n    validation_data=raw_val_ds,\n    epochs=epochs)",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.6643 - binary_accuracy: 0.6969 - val_loss: 0.6152 - val_binary_accuracy: 0.7694\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.5490 - binary_accuracy: 0.8008 - val_loss: 0.4986 - val_binary_accuracy: 0.8222\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.4454 - binary_accuracy: 0.8443 - val_loss: 0.4203 - val_binary_accuracy: 0.8472\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3782 - binary_accuracy: 0.8654 - val_loss: 0.3740 - val_binary_accuracy: 0.8614\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.3361 - binary_accuracy: 0.8784 - val_loss: 0.3452 - val_binary_accuracy: 0.8686\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.3054 - binary_accuracy: 0.8896 - val_loss: 0.3263 - val_binary_accuracy: 0.8710\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2815 - binary_accuracy: 0.8970 - val_loss: 0.3132 - val_binary_accuracy: 0.8734\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2626 - binary_accuracy: 0.9034 - val_loss: 0.3033 - val_binary_accuracy: 0.8770\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2458 - binary_accuracy: 0.9108 - val_loss: 0.2967 - val_binary_accuracy: 0.8782\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2311 - binary_accuracy: 0.9164 - val_loss: 0.2923 - val_binary_accuracy: 0.8796\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": "* After we train, we can see the performance on the test data.\n\n* Because our model produces logits, we will add a layer that applies sigmoidal activation.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "sigmoid_model = tf.keras.Sequential([\n  model,\n  layers.Activation('sigmoid')\n])\n\nsigmoid_model.compile(\n    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n)\n\n# Test it with `raw_test_ds`, which yields raw strings\nloss, accuracy = sigmoid_model.evaluate(raw_test_ds)\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 4s 5ms/step - loss: 0.3104 - accuracy: 0.8730\n",
            "Loss:  0.310350239276886\n",
            "Accuracy:  0.8730400204658508\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": "* Let's go back a bit to the vector representation of words in our model.\n\n* The vectors for each word are the weights of the `Embedding' layer.\n\n* This layer has dimensions `(vocabulary_size, embedding_dim)`.\n\n* The neural network learned the weights, i.e. it learned the vector representations of the words during training for the classification.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "embedding = model.layers[1]\nweights = embedding.get_weights()[0]\nweights.shape",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 16)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": "* So from where we started by representing words through integers, we end up representing each word as a point in a 16-dimensional space.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "for num in range(0, 5+1):\n    word = vectorize_layer.get_vocabulary()[num]\n    vec = weights[num]\n    print(word, num, vec)",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 0 [-0.00942429  0.00145162  0.00879299 -0.00617482 -0.01833293 -0.01385313\n",
            "  0.0061789   0.02245351  0.00614921 -0.00450425  0.00639349  0.01372139\n",
            "  0.00953717  0.00095779  0.01652874 -0.0068995 ]\n",
            "[UNK] 1 [-0.0311668   0.05364797  0.07109778  0.13044252  0.00565497  0.08358029\n",
            "  0.07035164  0.03607412  0.08179341  0.00629571  0.02268083  0.02624291\n",
            " -0.10860924  0.01658307 -0.01757364  0.03755026]\n",
            "the 2 [ 0.00751571 -0.02789821  0.00926731 -0.02363539 -0.06039637 -0.09125661\n",
            " -0.07214823 -0.06925654 -0.04588158 -0.01215377 -0.04814544 -0.03682938\n",
            "  0.02616501 -0.06814333  0.06467235 -0.00741657]\n",
            "and 3 [ 0.17544998 -0.19089417 -0.20559171 -0.27843451 -0.17202763 -0.17646806\n",
            " -0.20626883 -0.20862857 -0.1764741  -0.12801513 -0.2540157  -0.27600306\n",
            "  0.24608037 -0.2930975   0.18469746 -0.19249779]\n",
            "a 4 [ 0.15124586 -0.06932954 -0.06003192 -0.01276331 -0.08331891 -0.00961464\n",
            " -0.02208956 -0.05247774 -0.07412761 -0.05018733 -0.06016735 -0.04011693\n",
            " -0.01103566 -0.02208388  0.00852778 -0.04788162]\n",
            "of 5 [-0.05050444  0.01413051  0.01794358 -0.01096897  0.01073275  0.07030968\n",
            " -0.01485041  0.07324845  0.02461956  0.06463459  0.0461044   0.02719972\n",
            " -0.045023    0.00110167 -0.03491811  0.03686953]\n"
          ]
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "source": "* The use of vector representations of words is the basis of neural networks that manipulate language.\n\n* We in our simple example used a small data set to learn the vectors.\n\n* In practice, vector word representations are available that have been trained on huge text corpora.",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    }
  ]
}